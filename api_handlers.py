"""
API í•¸ë“¤ëŸ¬ ëª¨ë“ˆ (RAG ê¸°ë°˜)
"""

import logging
import re
from typing import Dict, Any, Set
from fastapi import HTTPException

from config import config
from model_manager import model_manager
from database_manager import database_manager
from medical_verification import MedicalFactChecker, format_verification_report

logger = logging.getLogger(__name__)

# ì˜ë£Œ ì •ë³´ ê²€ì¦ê¸° ì¸ìŠ¤í„´ìŠ¤
fact_checker = MedicalFactChecker()


class APIHandlers:
    """API í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤ (RAG ê¸°ë°˜)"""
    
    @staticmethod
    async def health_check() -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë° ì •ë³´ í™•ì¸"""
        try:
            system_info = {
                "status": "healthy",
                "system_info": {
                    "models": model_manager.get_system_info(),
                    "database": database_manager.get_database_info(),
                    "verification_system": {
                        "fact_checker_active": fact_checker is not None,
                        "verification_features": [
                            "ì˜ë£Œ ì •ë³´ ì •í™•ë„ í‰ê°€",
                            "ìœ„í—˜ë„ ë¶„ì„", 
                            "ê·¼ê±° ê¸°ë°˜ ê²€ì¦",
                            "ì „ë¬¸ê°€ ì¡°ì–¸ ì œê³µ"
                        ]
                    },
                    "configuration": config.to_dict()
                }
            }
            
            return system_info
            
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
    
    @staticmethod
    async def process_query(query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê³  ì˜ë£Œ ì •ë³´ ê²€ì¦ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (RAG ê¸°ë°˜)"""
        try:
            query = query.strip()
            if not query:
                raise HTTPException(status_code=400, detail="ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘: {query[:100]}...")
            
            # 1. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            similar_docs = database_manager.search_similar_documents(query)

            # 2. ì˜ë£Œ ì •ë³´ ê²€ì¦ ìˆ˜í–‰
            logger.info("ì˜ë£Œ ì •ë³´ ê²€ì¦ ì‹œì‘...")
            verification_result = await fact_checker.analyze_medical_claim(query, similar_docs)
            
            # 3. ì „ë¬¸ì ì¸ ê²€ì¦ ë³´ê³ ì„œ ìƒì„±
            answer = format_verification_report(verification_result, query)
            
            # 4. RAG ê¸°ë°˜ ì¶”ê°€ ì˜í•™ ì •ë³´ ìƒì„±: 'ë§ìŒ/í‹€ë¦¼'ì—ì„œë§Œ ìƒì„± (ì• ë§¤/ë¯¼ê°„ìš”ë²• ì œì™¸)
            if similar_docs and verification_result.judgment in ("ì˜¬ë°”ë¥¸ ì˜ë£Œ ì •ë³´", "ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì •ë³´"):
                additional_info = await APIHandlers._generate_additional_medical_info(query, similar_docs)
                fallback_needed = (not additional_info) or (additional_info.strip() == "ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                if fallback_needed:
                    # 4-1. ë³´ê°•: corrected_info ê¸°ë°˜ ìµœì†Œ 1~2ë¬¸ì¥ ìƒì„±
                    fallback_text = (verification_result.corrected_info or "").strip()
                    # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë¬¸êµ¬ëŠ” ì œì™¸í•˜ê³ , ë¬¸ì¥ ë‹¨ìœ„ ì •ë¦¬
                    def _sanitize_and_clip(txt: str) -> str:
                        import re as _re
                        s = (txt or "").strip()
                        # ì¼ë³¸ì–´/íƒœêµ­ì–´ ë“± ë¹„í•œê¸€ ìŠ¤í¬ë¦½íŠ¸ ì œê±° ë° ê³¼ë„ ê³µë°± ì •ë¦¬
                        s = _re.sub(r"[\u3040-\u30FF\u31F0-\u31FF\uFF65-\uFF9F\u0E00-\u0E7F]+", '', s)
                        s = _re.sub(r"\s{2,}", ' ', s)
                        # ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ í›„ 1~2ë¬¸ì¥ë§Œ ìœ ì§€
                        parts = [p.strip() for p in _re.split(r"[\.!?]\s+", s) if p.strip()]
                        picked = []
                        for p in parts:
                            if 12 <= len(p) <= 220:
                                picked.append(p)
                            if len(picked) >= 2:
                                break
                        if not picked and s:
                            picked = parts[:1] if parts else [s[:180]]
                        out = '. '.join([p.rstrip(' .') for p in picked])
                        if out and not _re.search(r"[\.!?]$", out):
                            out += '.'
                        # ì–´ì ˆ ì´ìƒ ê³µë°± êµì •
                        out = _re.sub(r'ìˆ\s*ìŠµ\s*ë‹ˆ\s*ë‹¤', 'ìˆìŠµë‹ˆë‹¤', out)
                        out = _re.sub(r'ì—†\s*ìŠµ\s*ë‹ˆ\s*ë‹¤', 'ì—†ìŠµë‹ˆë‹¤', out)
                        out = _re.sub(r'ë©\s*ë‹ˆ\s*ë‹¤', 'ë©ë‹ˆë‹¤', out)
                        return out.strip()

                    # ì§„ë‹¨/ë¬´ê´€ ë„ë©”ì¸ í‚¤ì›Œë“œ ì œì™¸ ì„¸íŠ¸
                    _diag = ['ë¯¸ë…¸ì „ì´íš¨ì†Œ', 'ast', 'got', 'ë¹Œë¦¬ë£¨ë¹ˆ', 'í˜ˆì²­', 'ìˆ˜ì¹˜', 'í™©ë‹¬', 'ê²€ì‚¬']
                    _other = ['ê³¤ì¶©', 'í¡í˜ˆ', 'ìœ ì¶©', 'ì„±ì¶©', 'ë²¼ë£©', 'ì§„ë“œê¸°', 'ëª¨ê¸°']

                    if fallback_text and 'ê´€ë ¨' not in fallback_text:
                        tmp = _sanitize_and_clip(fallback_text)
                        low = tmp.lower()
                        if not any(k in low for k in _diag) and not any(k in low for k in _other):
                            additional_info = tmp
                    else:
                        # 4-2. ê·¼ê±° ë¬¸ì„œì—ì„œ ì§ì ‘ 1~2ë¬¸ì¥ ì¶”ì¶œ (ì§ˆì˜ í† í°ê³¼ì˜ ê´€ë ¨ì„± ê¸°ì¤€)
                        try:
                            import re as _re
                            q_tokens = set(_re.findall(r"[ê°€-í£A-Za-z0-9]+", (query or '').lower()))
                            content = (similar_docs[0].get('content') or '')
                            sentences = [s.strip() for s in _re.split(r"[\.!?]\s+", content) if s.strip()]
                            picked = []
                            for s in sentences:
                                s_lower = s.lower()
                                s_tokens = set(_re.findall(r"[ê°€-í£A-Za-z0-9]+", s_lower))
                                overlap = len(q_tokens & s_tokens) / max(1, len(q_tokens))
                                # Aí˜•ê°„ì—¼ ë§¥ë½ ê°•ì œ ë° B/Cí˜• ê°„ì—¼ ë°°ì œ
                                has_hep_a = any(tok in s_lower for tok in ['aí˜•ê°„ì—¼', 'aí˜• ê°„ì—¼', 'aí˜•ê°ì—¼', 'hepatitis a', 'hav'])
                                has_hep_bc = any(tok in s_lower for tok in ['bí˜•ê°„ì—¼', 'b í˜• ê°„ì—¼', 'bí˜• ê°„ì—¼', 'hbv', 'hepatitis b',
                                                                              'cí˜•ê°„ì—¼', 'c í˜• ê°„ì—¼', 'cí˜• ê°„ì—¼', 'hcv', 'hepatitis c'])
                                has_diag = any(k in s_lower for k in _diag)
                                has_other = any(k in s_lower for k in _other)
                                if overlap >= 0.25 and 12 <= len(s) <= 220 and has_hep_a and not has_hep_bc and not has_diag and not has_other:
                                    picked.append(s)
                                if len(picked) >= 2:
                                    break
                            if picked:
                                tmp = '. '.join([p.rstrip(' .') for p in picked])
                                if tmp and not _re.search(r"[\.!?]$", tmp):
                                    tmp += '.'
                                additional_info = _sanitize_and_clip(tmp)
                        except Exception:
                            pass

                if additional_info:
                    answer += f"\n\nğŸ“š ì¶”ê°€ ì˜í•™ ì •ë³´:\n{additional_info}"

            # 4-Î². í•µì‹¬ ì„¤ëª… í’ˆì§ˆ ë³´ê°•: ë„ˆë¬´ ì¼ë°˜ì ì´ë©´ RAGë¡œ ëŒ€ì²´ ìƒì„±
            try:
                answer = APIHandlers._maybe_improve_core_explanation(answer, query, similar_docs, verification_result.judgment)
            except Exception:
                pass

            # 4-Î³. ì¶”ê°€ ì„¤ëª… ë¹„ê±°ë‚˜ ë¹ˆì•½í•  ë•Œ ë³´ê°•
            try:
                if verification_result.judgment in ("ì˜¬ë°”ë¥¸ ì˜ë£Œ ì •ë³´", "ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì •ë³´"):
                    import re as _re
                    m_add = _re.search(r"ğŸ“š\s*ì¶”ê°€\s*ì˜í•™\s*ì •ë³´\s*:\s*([\s\S]*)$", answer)
                    current_add = (m_add.group(1).strip() if m_add else '')
                    if not current_add or len(current_add) < 20 or 'ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤' in current_add:
                        better_add = await APIHandlers._generate_additional_medical_info(query, similar_docs)
                        if better_add and better_add != 'ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.':
                            if m_add:
                                answer = answer[:m_add.start(1)] + better_add + answer[m_add.end(1):]
                            else:
                                answer += f"\n\nğŸ“š ì¶”ê°€ ì˜í•™ ì •ë³´:\n{better_add}"
            except Exception:
                pass
            
            sources = verification_result.evidence_sources
            logger.info(f"API ì‘ë‹µì—ì„œ sources ê°œìˆ˜: {len(sources)}")
            
            # 5. ì„¹ì…˜ êµ¬ì¡° ìƒì„± (í”„ë¡ íŠ¸ ì§ì ‘ ì‚¬ìš©)
            # ì •ì±…: ì˜¬ë°”ë¥¸/ì˜¬ë°”ë¥´ì§€ ì•Šì€ â†’ í•µì‹¬/ê·¼ê±°/ì¶”ê°€ í‘œì‹œ, íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€/ë¯¼ê°„ìš”ë²• â†’ ê·¼ê±°ë§Œ
            core_section = ""
            reason_section = verification_result.reason or ""
            additional_section = ""
            try:
                if verification_result.judgment in ("ì˜¬ë°”ë¥¸ ì˜ë£Œ ì •ë³´", "ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì •ë³´"):
                    # í•µì‹¬ ì„¤ëª…: ê¸°ì¡´ answerì—ì„œ ì¶”ì¶œ(ë¼ë²¨ ê¸°ë°˜) ë˜ëŠ” RAG ìƒì„± í´ë°±
                    import re as _re
                    m_core = _re.search(r"í•µì‹¬\s*ì„¤ëª…\s*\n([\s\S]*?)(?:\n\n|\nğŸ’¡|\níŒë‹¨\s*ê·¼ê±°|$)", answer)
                    core_section = (m_core.group(1).strip() if m_core else "")
                    # ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜ ë¬¸êµ¬ë©´ RAG ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„
                    def _is_generic(txt: str) -> bool:
                        t = (txt or '').strip()
                        return (not t) or len(t) < 15 or t.endswith('í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.') or ('ì •ë³´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤' in t)
                    if _is_generic(core_section):
                        try:
                            # ë™ê¸° ì»¨í…ìŠ¤íŠ¸: ê°„ë‹¨ í›„ë³´ ì¶”ì¶œë¡œ ëŒ€ì²´
                            import re as _re2
                            q_tokens = APIHandlers._extract_core_keywords(query)
                            q_low = query.lower()
                            topic_hints = set()
                            if any(t in q_low for t in ['ë…ê¸°','dengue']):
                                topic_hints.update(['ë…ê¸°','dengue','ì´ì§‘íŠ¸ìˆ²ëª¨ê¸°','í°ì¤„ìˆ²ëª¨ê¸°','aedes','ëª¨ê¸°','ë°”ì´ëŸ¬ìŠ¤'])
                            if any(t in q_low for t in ['aí˜•ê°„ì—¼','aí˜• ê°„ì—¼','aí˜•ê°ì—¼','hepatitis a','hav','ê°„ì—¼']):
                                topic_hints.update(['aí˜•ê°„ì—¼','aí˜• ê°„ì—¼','hepatitis a','hav','ê°„ì—¼','ë°”ì´ëŸ¬ìŠ¤'])
                            diagnostic_block = ['ë¯¸ë…¸ì „ì´íš¨ì†Œ','ast','got','ë¹Œë¦¬ë£¨ë¹ˆ','í˜ˆì²­','ìˆ˜ì¹˜','í™©ë‹¬','ê²€ì‚¬','igm','igg','í•­ì²´','pcr','rt-pcr','ìœ ì „ì','ê²€ì²´','í˜ˆì•¡','ë‡Œì²™ìˆ˜ì•¡','serology','í•­ì›']
                            other_disease = ['bí˜•ê°„ì—¼','hepatitis b','hbv','cí˜•ê°„ì—¼','hepatitis c','hcv']
                            cands = []
                            for doc in (similar_docs or [])[:2]:
                                content = (doc.get('content') or '')
                                for s in _re2.split(r"[\.!?]\s+", content):
                                    s = s.strip()
                                    if not s:
                                        continue
                                    s_low = s.lower()
                                    if topic_hints and not any(h in s_low for h in topic_hints):
                                        continue
                                    if any(k in s_low for k in diagnostic_block) or any(k in s_low for k in other_disease):
                                        continue
                                    s_tokens = set(_re2.findall(r"[ê°€-í£A-Za-z0-9]+", s_low))
                                    overlap = len(q_tokens & s_tokens) / max(1, len(q_tokens))
                                    if overlap >= 0.25 and 20 <= len(s) <= 220:
                                        cands.append((overlap, s))
                            if cands:
                                cands.sort(key=lambda x: x[0], reverse=True)
                                new_core = cands[0][1]
                                if new_core and new_core[-1] not in '.!?':
                                    new_core += '.'
                                core_section = new_core
                        except Exception:
                            pass
                    # ì¶”ê°€ ì„¤ëª…: answerì—ì„œ ë¸”ë¡ ì¶”ì¶œ(ìˆìœ¼ë©´) ë˜ëŠ” ë¹„ì—ˆìœ¼ë©´ ìœ„ ìƒì„±ê°’ ì‚¬ìš©
                    import re as _re3
                    m_add = _re3.search(r"ğŸ“š\s*ì¶”ê°€\s*ì˜í•™\s*ì •ë³´\s*:\s*([\s\S]*)$", answer)
                    additional_section = (m_add.group(1).strip() if m_add else "")
            except Exception:
                pass

            sections = {
                "core": core_section if verification_result.judgment in ("ì˜¬ë°”ë¥¸ ì˜ë£Œ ì •ë³´", "ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì •ë³´") else "",
                "reason": reason_section,
                "additional": additional_section if verification_result.judgment in ("ì˜¬ë°”ë¥¸ ì˜ë£Œ ì •ë³´", "ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì •ë³´") else ""
            }
            
            # 6. ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
            system_info = model_manager.get_system_info()
            system_info.update(database_manager.get_database_info())

            response = {
                "answer": answer,
                "sources": sources,
                "sections": sections,
                "verification_details": {
                    "confidence_score": verification_result.confidence_score,
                    "risk_level": verification_result.risk_level,
                    "is_accurate": verification_result.is_accurate,
                    "warnings": verification_result.warnings,
                    "matched_keywords": getattr(verification_result, "matched_keywords", []),
                    "triggered_rules": getattr(verification_result, "triggered_rules", []),
                    "judgment": getattr(verification_result, "judgment", ""),
                    "reason": getattr(verification_result, "reason", "")
                },
                "system_info": system_info
            }
            
            logger.info("ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ")
            return response
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")

    @staticmethod
    async def _generate_additional_medical_info(query: str, context_docs: list) -> str:
        """RAG ê¸°ë°˜ ì¶”ê°€ ì˜í•™ ì •ë³´ ìƒì„± - ë™ì  í•„í„°ë§ ê°•í™”"""
        
        try:
            if not context_docs:
                return "ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # 1. ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            query_keywords = APIHandlers._extract_core_keywords(query)
            query_lower = query.lower()
            
            # ì¶”ê°€: ì˜ë„/ì£¼ì œë³„ í‚¤ì›Œë“œ ì„¸íŠ¸ (ë™ì  í•„í„° ê°•í™”)
            complication_keywords = ['í•©ë³‘ì¦', 'ê¸°ì•µ-ë°”ë ˆì¦í›„êµ°', 'ê¸‰ì„± ì‹ ë¶€ì „', 'ë‹´ë‚­ì—¼', 'ì·Œì¥ì—¼', 'í˜ˆê´€ì—¼', 'ê´€ì ˆì—¼']
            # ì§ˆí™˜ í† í°: ì¿¼ë¦¬ì—ì„œ ë™ì ìœ¼ë¡œ í™•ì¥ (ê°„ì—¼/ë…ê¸° ë“±)
            base_hep = ['aí˜•ê°„ì—¼', 'ì—ì´í˜•ê°„ì—¼', 'a í˜• ê°„ì—¼', 'aí˜• ê°„ì—¼', 'ê°„ì—¼', 'hepatitis a']
            dengue_terms = ['ë…ê¸°', 'dengue']
            hepatitis_keywords = base_hep + ([] if not any(t in query_lower for t in dengue_terms) else dengue_terms)
            other_disease_keywords = [
                'ì „ì—¼ì„± ë‹¨í•µêµ¬ì¦', 'í¸ë„ì—¼', 'ì¸ë‘ì—¼', 'ì—°ì‡„ìƒêµ¬ê· ', 'ìš”ê´€ì•”', 'ì„±ë¶„í™”ì´ìƒ',
                # Aí˜•ê°„ì—¼ ë§¥ë½ì—ì„œ ë°°ì œí•  B/Cí˜• ê°„ì—¼ ê´€ë ¨ ìš©ì–´
                'bí˜•ê°„ì—¼', 'b í˜• ê°„ì—¼', 'bí˜• ê°„ì—¼', 'bí˜• ê°ì—¼', 'hbv', 'hepatitis b',
                'cí˜•ê°„ì—¼', 'c í˜• ê°„ì—¼', 'cí˜• ê°„ì—¼', 'cí˜• ê°ì—¼', 'hcv', 'hepatitis c'
            ]
            diagnostic_keywords = [
                'ë¯¸ë…¸ì „ì´íš¨ì†Œ', 'ast', 'got', 'ë¹Œë¦¬ë£¨ë¹ˆ', 'í˜ˆì²­', 'ìˆ˜ì¹˜', 'í™©ë‹¬', 'ê²€ì‚¬',
                # ì§ˆí™˜ ì¼ë°˜í™”: ê²€ì‚¬/ê²€ì²´/ë¶„ìì§„ë‹¨/í•­ì²´ ìš©ì–´ëŠ” ì¶”ê°€ ì„¤ëª…ì—ì„œ ë°°ì œ
                'igm', 'igg', 'í•­ì²´', 'pcr', 'rt-pcr', 'ìœ ì „ì', 'ê²€ì²´', 'í˜ˆì•¡', 'ë‡Œì²™ìˆ˜ì•¡', 'serology', 'í•­ì›'
            ]
            # ì¹˜ë£Œ ê´€ë ¨ ìš©ì–´(í—ˆìœ„ ê¶Œê³  ë°©ì§€): ë¶€ì •ì  ë§¥ë½(ì—†ìŒ/ê¶Œì¥X/íš¨ê³¼X) ì—†ì´ ë“±ì¥ ì‹œ ì œì™¸
            treatment_keywords = ['í•­ìƒì œ', 'í•­ë°”ì´ëŸ¬ìŠ¤', 'í•­ë¹„íƒ€ë¯¼', 'í•­ë¹„íƒ€ë¯¼ì œ', 'íŠ¹íš¨ì•½', 'íŠ¹íš¨ ì¹˜ë£Œì œ', 'ì¹˜ë£Œì œ']
            negative_cues = ['ì—†', 'ì•„ë‹˜', 'ê¶Œì¥ë˜ì§€ ì•Š', 'ë¹„ê¶Œì¥', 'íš¨ê³¼ ì—†', 'ê·¼ê±° ì—†']
            # Aí˜•ê°„ì—¼ ë§¥ë½ì—ì„œ ëª…ë°±íˆ ì˜ëª»ëœ ì£¼ì¥ í•„í„°
            false_claim_patterns = [
                r'ì „ì—¼ë˜\s*ì§€\s*ì•Š',
                r'ë°±ì‹ \s*ì€?\s*í•„ìš”\s*ì—†',
                r'íŠ¹íš¨.*ì¹˜ë£Œì œ.*ìˆ'
            ]
            
            # 2. ë¬¸ì„œ ë‚´ìš©ì—ì„œ ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ë§Œ í•„í„°ë§
            relevant_content = []
            for doc in context_docs[:2]:
                content = doc.get('content', '')
                content_lower = content.lower()
                
                # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë¬¸ì¥ë§Œ ì¶”ì¶œ
                sentences = content.split('.')
                relevant_sentences = []
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    sentence_lower = sentence.lower()
                    
                    # í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë§Œ ì„ íƒ
                    keyword_matches = sum(1 for kw in query_keywords if kw in sentence_lower)
                    
                    # ë¬´ê´€í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì œì™¸
                    has_irrelevant = any(kw in sentence_lower for kw in diagnostic_keywords)
                    has_other_disease = any(kw.lower() in sentence_lower for kw in other_disease_keywords)
                    
                    # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                    relevance_score = keyword_matches / max(1, len(query_keywords))

                    # ì˜ë„ ê¸°ë°˜ ê°•í™” ì¡°ê±´: í•©ë³‘ì¦ ì§ˆì˜ì¼ ë•Œ
                    if 'í•©ë³‘ì¦' in query_lower:
                        # ê°„ì—¼/í—¤íŒŒíƒ€ì´í‹°ìŠ¤ ë§¥ë½ í™•ì¸
                        has_hepatitis = any(kw in sentence_lower for kw in hepatitis_keywords)
                        # í•©ë³‘ì¦ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
                        has_complication = any(kw in sentence_lower for kw in complication_keywords)
                        # ì¿¼ë¦¬ ë¬¸ì¥ ìì²´ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ë³µí•œ ë¬¸ì¥ ì œì™¸ (ê±°ì˜ ë™ì¼í•œ ê²½ìš°)
                        q_tokens = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", query_lower))
                        s_tokens = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", sentence_lower))
                        overlap_ratio = len(q_tokens & s_tokens) / max(1, len(q_tokens))

                        if (not has_complication and 'í•©ë³‘ì¦' not in sentence_lower) or not has_hepatitis:
                            continue
                        if has_other_disease:
                            continue
                        if has_irrelevant:
                            continue
                        # ë„ˆë¬´ ë™ì¼í•œ ë¬¸ì¥(ê±°ì˜ ë³µë¶™)ì€ ì œì™¸í•˜ì—¬ ë¶€ì—°ì„¤ëª… ë‹¤ì–‘ì„± í™•ë³´
                        if overlap_ratio > 0.95:
                            continue
                    
                    # ì¹˜ë£Œ ê´€ë ¨ ìš©ì–´ê°€ ë¶€ì • ë§¥ë½ ì—†ì´ í¬í•¨ë˜ë©´ ì œì™¸
                    has_treatment = any(kw in sentence_lower for kw in treatment_keywords)
                    negated_treatment = any(cue in sentence_lower for cue in negative_cues)
                    if has_treatment and not negated_treatment:
                        continue
                    # ëª…ë°±í•œ í—ˆìœ„ ì£¼ì¥ íŒ¨í„´ ì œì™¸
                    if any(re.search(pat, sentence_lower) for pat in false_claim_patterns):
                        continue
                    # ë¶ˆì™„ì „/ìƒëµë¶€í˜¸ ë¬¸ì¥ ì œì™¸
                    if ('...' in sentence) or ('â€¦' in sentence) or re.search(r'[\.]{2,}$', sentence):
                        continue
                    if re.search(r'(ìœ¼ë¡œ|ë¡œ|ë©°|ê³ |ë“±)$', sentence):
                        continue
                    # ê´€ë ¨ì„±ì´ ë†’ê³  ë¬´ê´€í•œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ë¬¸ì¥ë§Œ í¬í•¨
                    if relevance_score >= 0.3 and not has_irrelevant and not has_other_disease:
                        relevant_sentences.append(sentence)
                
                if relevant_sentences:
                    # ë¬¸ì¥ ìˆ˜ë¥¼ 2ê°œë¡œ ì œí•œí•˜ì—¬ ì§‘ì¤‘ë„ í–¥ìƒ
                    clipped = relevant_sentences[:2]
                    relevant_content.append('. '.join(clipped) + '.')
            
            # 3. ê´€ë ¨ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ
            if not relevant_content:
                    return "ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # 4. ê²€ì¦ëœ ê·¼ê±° ë¬¸ì¥ ê¸°ë°˜ ì•ˆì „ ìš”ì•½ ë°˜í™˜ (ë¬¸ì¥ ë‹¨ìœ„, ë‹¤ë¬¸ì¥ í—ˆìš©)
            if relevant_content:
                raw_text = ' '.join(relevant_content)
                # ë¦¬ìŠ¤íŠ¸ ê¸°í˜¸/ê³¼ë„í•œ ê³µë°± ì œê±°
                cleaned = re.sub(r"[â€¢\-\u2022]+", ' ', raw_text)
                # ê³¼ë„/ë¹„ì •ìƒ ê³µë°± ì •ê·œí™” (ì¤‘ê°„ì— ë„ì–´ì“°ê¸° ê¹¨ì§ ë°©ì§€)
                cleaned = re.sub(r"\s{2,}", ' ', cleaned)
                cleaned = re.sub(r"(\S)\s{1}(\S)", r"\1 \2", cleaned)  # ë‹¨ì¼ ê³µë°±ë§Œ ìœ ì§€
                cleaned = cleaned.strip()
                # ì¼ë³¸ì–´/íƒœêµ­ì–´ ë“± ë¹„í•œê¸€ ìŠ¤í¬ë¦½íŠ¸ ì œê±°
                cleaned = re.sub(r"[\u3040-\u30FF\u31F0-\u31FF\uFF65-\uFF9F\u0E00-\u0E7F]+", '', cleaned)

                # ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  í›„ 3~4ë¬¸ì¥ìœ¼ë¡œ ì¬ì¡°í•©
                sentences = [s.strip() for s in re.split(r"[\.?!]\s+", cleaned) if s.strip()]
                picked = []
                for s in sentences:
                    if 10 <= len(s) <= 220:
                        picked.append(s)
                    if len(picked) >= 4:
                        break
                if not picked:
                    picked = sentences[:2]
                # ë¬¸ì¥ ì¢…ê²° ë³´ì • ë° ê³¼ë„ ê³µë°± ì œê±°
                result_text = '. '.join([s.rstrip(' .') for s in picked])
                # ì¤‘ë³µ ì ‘ë‘ì–´(ì˜ˆ: "í•©ë³‘ì¦ Aí˜•ê°„ì—¼ì˜ í•©ë³‘ì¦") ì œê±°
                result_text = re.sub(r'^í•©ë³‘ì¦\s+(?=Aí˜•ê°„ì—¼ì˜\s+í•©ë³‘ì¦)', '', result_text)
                if result_text and not re.search(r"[\.!?]$", result_text):
                    result_text += '.'
                # ì–´ì ˆ ì´ìƒ ê³µë°± êµì •(ìˆ ìŠµ ë‹ˆë‹¤ ë“±) ë° ê³¼ë„ ê³µë°± ì œê±°
                result_text = re.sub(r'ìˆ\s*ìŠµ\s*ë‹ˆ\s*ë‹¤', 'ìˆìŠµë‹ˆë‹¤', result_text)
                result_text = re.sub(r'ì—†\s*ìŠµ\s*ë‹ˆ\s*ë‹¤', 'ì—†ìŠµë‹ˆë‹¤', result_text)
                result_text = re.sub(r'ë©\s*ë‹ˆ\s*ë‹¤', 'ë©ë‹ˆë‹¤', result_text)
                result_text = re.sub(r"\s{2,}", ' ', result_text).strip()
                return result_text
                
        except Exception as e:
            logger.warning(f"ì¶”ê°€ ì˜í•™ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ê´€ë ¨ëœ ì˜í•™ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    @staticmethod
    async def _generate_core_explanation(query: str, context_docs: list) -> str:
        """RAG ê¸°ë°˜ í•µì‹¬ ì„¤ëª… ìƒì„± (1-2ë¬¸ì¥, ì§ì ‘ì /ì˜í•™ì  ì„¤ëª…)"""
        try:
            if not context_docs:
                return ""
            # ë¬¸ì¥ í›„ë³´ ì¶”ì¶œ: ì§ˆì˜ í‚¤ì›Œë“œì™€ì˜ ê²¹ì¹¨ + Aí˜•ê°„ì—¼ ë§¥ë½ ê°•ì œ + ì§„ë‹¨/íƒ€ì§ˆí™˜ ë°°ì œ
            q_tokens = APIHandlers._extract_core_keywords(query)
            hepatitis_a_terms = ['aí˜•ê°„ì—¼', 'aí˜• ê°„ì—¼', 'aí˜•ê°ì—¼', 'hepatitis a', 'hav', 'ê°„ì—¼']
            diagnostic_keywords = ['ë¯¸ë…¸ì „ì´íš¨ì†Œ', 'ast', 'got', 'ë¹Œë¦¬ë£¨ë¹ˆ', 'í˜ˆì²­', 'ìˆ˜ì¹˜', 'í™©ë‹¬', 'ê²€ì‚¬']
            other_disease_keywords = ['bí˜•ê°„ì—¼','b í˜• ê°„ì—¼','bí˜• ê°„ì—¼','hbv','hepatitis b','cí˜•ê°„ì—¼','c í˜• ê°„ì—¼','cí˜• ê°„ì—¼','hcv','hepatitis c',
                                      'ì „ì—¼ì„± ë‹¨í•µêµ¬ì¦','í¸ë„ì—¼','ì¸ë‘ì—¼','ì—°ì‡„ìƒêµ¬ê· ','ìš”ê´€ì•”','ì„±ë¶„í™”ì´ìƒ']

            import re as _re
            candidates = []
            for doc in context_docs[:2]:
                content = (doc.get('content') or '')
                for s in _re.split(r"[\.!?]\s+", content):
                    s = s.strip()
                    if not s:
                        continue
                    s_lower = s.lower()
                    has_hep_a = any(t in s_lower for t in hepatitis_a_terms)
                    has_diag = any(k in s_lower for k in diagnostic_keywords)
                    has_other = any(k in s_lower for k in other_disease_keywords)
                    if not has_hep_a or has_diag or has_other:
                        continue
                    s_tokens = set(_re.findall(r"[ê°€-í£A-Za-z0-9]+", s_lower))
                    overlap = len(q_tokens & s_tokens) / max(1, len(q_tokens))
                    if overlap >= 0.25 and 20 <= len(s) <= 220:
                        candidates.append((overlap, s))
            if not candidates:
                return ""
            # ê²¹ì¹¨ ë†’ì€ ìˆœìœ¼ë¡œ 1-2ë¬¸ì¥ ì„ íƒ
            candidates.sort(key=lambda x: x[0], reverse=True)
            picked = [candidates[0][1]]
            if len(candidates) > 1 and candidates[1][0] >= 0.3:
                picked.append(candidates[1][1])
            core = '. '.join([p.rstrip(' .') for p in picked])
            if core and core[-1] not in '.!?':
                core += '.'
            core = re.sub(r'ìˆ\s*ìŠµ\s*ë‹ˆ\s*ë‹¤', 'ìˆìŠµë‹ˆë‹¤', core)
            core = re.sub(r'ì—†\s*ìŠµ\s*ë‹ˆ\s*ë‹¤', 'ì—†ìŠµë‹ˆë‹¤', core)
            core = re.sub(r'ë©\s*ë‹ˆ\s*ë‹¤', 'ë©ë‹ˆë‹¤', core)
            return core
        except Exception as e:
            logger.warning(f"í•µì‹¬ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    @staticmethod
    def _maybe_improve_core_explanation(answer: str, query: str, context_docs: list, judgment: str) -> str:
        """í•µì‹¬ ì„¤ëª…ì´ ì¼ë°˜ì ì´ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ RAGë¡œ ëŒ€ì²´ ìƒì„±í•˜ì—¬ ì¹˜í™˜"""
        try:
            if judgment not in ("ì˜¬ë°”ë¥¸ ì˜ë£Œ ì •ë³´", "ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì •ë³´"):
                return answer
            import re as _re
            # í•µì‹¬ ë¸”ë¡ ì¶”ì¶œ: 'í•µì‹¬ ì„¤ëª…' ë‹¤ìŒ ë¸”ë¡(ğŸ’¡ ë˜ëŠ” ë¹ˆì¤„ ì „ê¹Œì§€)
            m = _re.search(r"í•µì‹¬\s*ì„¤ëª…\s*\n([\s\S]*?)(?:\n\n|\nğŸ’¡|\níŒë‹¨\s*ê·¼ê±°|$)", answer)
            current_core = (m.group(1).strip() if m else '')
            def is_generic(txt: str) -> bool:
                t = (txt or '').strip()
                if not t:
                    return True
                if len(t) < 15:
                    return True
                return t.endswith('í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.') or ('ì •ë³´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤' in t)
            if is_generic(current_core):
                # ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ core ìƒì„± í˜¸ì¶œì€ async ë¶ˆê°€ â†’ ê°„ë‹¨ í›„ë³´ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´
                # ë¹„ë™ê¸° í•¨ìˆ˜ëŠ” ìƒìœ„ì—ì„œ í˜¸ì¶œí•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ê¸°ì¡´ ë¬¸ì„œì—ì„œ ì¶”ì¶œ
                try:
                    import re as _re2
                    q_tokens = APIHandlers._extract_core_keywords(query)
                    hepatitis_a_terms = ['aí˜•ê°„ì—¼','aí˜• ê°„ì—¼','aí˜•ê°ì—¼','hepatitis a','hav','ê°„ì—¼']
                    diagnostic_keywords = ['ë¯¸ë…¸ì „ì´íš¨ì†Œ','ast','got','ë¹Œë¦¬ë£¨ë¹ˆ','í˜ˆì²­','ìˆ˜ì¹˜','í™©ë‹¬','ê²€ì‚¬']
                    other_disease_keywords = ['bí˜•ê°„ì—¼','b í˜• ê°„ì—¼','bí˜• ê°„ì—¼','hbv','hepatitis b','cí˜•ê°„ì—¼','c í˜• ê°„ì—¼','cí˜• ê°„ì—¼','hcv','hepatitis c']
                    cands = []
                    for doc in (context_docs or [])[:2]:
                        content = (doc.get('content') or '')
                        for s in _re2.split(r"[\.!?]\s+", content):
                            s = s.strip()
                            if not s:
                                continue
                            s_low = s.lower()
                            if not any(t in s_low for t in hepatitis_a_terms):
                                continue
                            if any(k in s_low for k in diagnostic_keywords) or any(k in s_low for k in other_disease_keywords):
                                continue
                            s_tokens = set(_re2.findall(r"[ê°€-í£A-Za-z0-9]+", s_low))
                            overlap = len(q_tokens & s_tokens) / max(1, len(q_tokens))
                            if overlap >= 0.25 and 20 <= len(s) <= 220:
                                cands.append((overlap, s))
                    if cands:
                        cands.sort(key=lambda x: x[0], reverse=True)
                        new_core = cands[0][1]
                        if new_core and new_core[-1] not in '.!?':
                            new_core += '.'
                        # ì¹˜í™˜
                        if m:
                            answer = answer[:m.start(1)] + new_core + answer[m.end(1):]
                        else:
                            # í•µì‹¬ ë¸”ë¡ì´ ë¹„ì •ìƒì¼ ë•Œ ì•ˆì „ ì‚½ì…
                            answer = re.sub(r"í•µì‹¬\s*ì„¤ëª…\s*\n", f"í•µì‹¬ ì„¤ëª…\n{new_core}\n\n", answer)
                except Exception:
                    pass
            return answer
        except Exception:
            return answer

    @staticmethod
    def _extract_core_keywords(text: str) -> Set[str]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tokens = set()
        for tok in re.findall(r"[ê°€-í£A-Za-z0-9]+", text.lower()):
            if len(tok) >= 2:
                tokens.add(tok)
        return tokens

    @staticmethod
    def _is_response_relevant(query: str, response: str) -> bool:
        """ë™ì  ì‘ë‹µ ê´€ë ¨ì„± ì²´í¬"""
        query_lower = query.lower()
        response_lower = response.lower()
        
        # ì§ˆë¬¸ê³¼ ì‘ë‹µì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        query_tokens = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", query_lower))
        response_tokens = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", response_lower))
        
        # í‚¤ì›Œë“œ ê²¹ì¹¨ë¥  ê³„ì‚°
        overlap = len(query_tokens & response_tokens)
        overlap_ratio = overlap / max(1, len(query_tokens))
        
        # ìµœì†Œ 80% ì´ìƒ ê²¹ì³ì•¼ ê´€ë ¨ì„± ìˆë‹¤ê³  íŒë‹¨
        return overlap_ratio >= 0.8

    @staticmethod
    def _extract_preserve_terms(text: str) -> list:
        """ì§ˆë¬¸ ë‚´ì—ì„œ ë°˜ë“œì‹œ ë³´ì¡´í•´ì•¼ í•  í‘œê¸°(ì•½ì–´/ìœ í˜•)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        terms = set()
        try:
            allowed = getattr(config.verification, 'ALLOWED_MEDICAL_ABBREVIATIONS', []) or []
            for abbr in allowed:
                if abbr and abbr in text:
                    terms.add(abbr)
        except Exception:
            pass
        # ì¼ë°˜ íŒ¨í„´: A/B/C/...í˜•
        for m in re.findall(r"\b([A-Za-z])í˜•\b", text):
            terms.add(f"{m.upper()}í˜•")
        return sorted(terms)


# API í•¸ë“¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
api_handlers = APIHandlers()
